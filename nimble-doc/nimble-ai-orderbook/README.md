---
cover: >-
  https://images.unsplash.com/photo-1544716278-ca5e3f4abd8c?crop=entropy&cs=srgb&fm=jpg&ixid=M3wxOTcwMjR8MHwxfHNlYXJjaHw2fHxib29rfGVufDB8fHx8MTcxNDA3MTI4Mnww&ixlib=rb-4.0.3&q=85
coverY: 0
---

# ðŸ“– Nimble AI OrderBook

The network supports both AI training and inferences. Successful AI training hinges on three critical resources: compute power (GPUs), data, and developers. Each of these pillars faces its own set of challengesâ€”from accessibility and scalability to efficiency. Similarly, AI inferences relies on computer power and application calls.

We call the whole system "AI Orderbook" as a one-stop shop that connects everything together on chain, covering both training and inference.

Nimble Matrix represents complex data structures as standardized DSLs, flowing across the network. Matching, pricing, and transactions are sequentially executed for each Nimble Matrix on the AI Orderbook.

Please refer to [Nimble architecture](https://docs.nimble.technology/nimble-doc/nimble-architecture) for more detailed design on AI Orderbook and Nimble Matrix. The following sections provide an overview on orchestration among compute, data and developers.

